from langchain_huggingface import HuggingFaceEmbeddings
from dotenv import load_dotenv
from pydantic import BaseModel, Json
import json
import os
from supabase import create_client, Client
from langchain_community.vectorstores import SupabaseVectorStore
from user import get_client_session 

"""
model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

embedding = hf.embed_query("sample text")
print(len(embedding))
print(type(embedding))  #list of floats

"""
class Document(BaseModel):
    #no need for id
    session_id: str
    # no need for inserted_at because it is auto-generated by supabase
    data: Json
    subject: str
    topic: str
    question_type: str
    topic_id: int 

class Topic(BaseModel):
    #no need id 
    topic: str
    subject: str | None = None
    embedding: list[float]
    session_id: str
    #inserted_at: no need

def _get_topic_id(supabase, session_id, topic_name, hf, subject=None):
    #query for topic in Topic table. create if not exists
    if subject is None:
        topic = supabase.table("topic").select("*").eq("topic", topic_name).eq("session_id", session_id).execute()
    else:
        topic = supabase.table("topic").select("*").eq("topic", topic_name).eq("subject", subject).eq("session_id", session_id).execute()
    if not(topic.data):
        embedding= hf.embed_query(topic_name)
        topic = Topic(topic=topic_name, embedding=embedding, subject=subject, session_id=session_id)
        result =supabase.table("topic").insert(topic.model_dump()).execute()
        topic_id = result.data[0]["id"]
        return topic_id
    else:
        return topic.data[0]["id"] #topic is unique per session_id 

def insert_document(supabase,hf:HuggingFaceEmbeddings, data, subject, topic, question_type, session_id) -> None:
    topic_id= _get_topic_id(supabase=supabase, session_id=session_id, topic_name=topic, hf=hf, subject=subject)
    document= Document(
        session_id=session_id,
        data=data,
        subject=subject,
        topic=topic,
        question_type=question_type,
        topic_id=topic_id
    )
    supabase.table("document").insert(document.model_dump()).execute()

def query_by_topic(supabase,topic_name, query_subject, hf:HuggingFaceEmbeddings):
    """
    Query documents by topic name and session_id.
    Returns a list of topic ids.
    """
    session_id = get_client_session()
    embedding_vector = hf.embed_query(topic_name)
    results= supabase.rpc("match_documents", {
    "query_embedding": embedding_vector,
    "match_count": 5,
    "query_session_id": session_id,
    "query_subject": query_subject
    }).execute()
    return [result["id"] for result in results.data]

def delete_data_by_session(supabase,session_id:str) -> None:
    """
    Delete all data associated with a session_id.
    """
    supabase.table("document").delete().eq("session_id", session_id).execute()
    supabase.table("topic").delete().eq("session_id", session_id).execute()


###################              tests          ####################

def test_insert_document(supabase):
    """
    Test the insert_document function by inserting a sample document
    and checking if it exists in the database.
    try changing the topic to a different one to see if it creates a new topic
    """
    session_id = get_client_session()
    data = {"Question": "What is the capital of France?", "Answer": "Paris"}
    binary = json.dumps(data).encode('utf-8')
    subject = "Geography"
    topic = "Capitals" 
    question_type = "MCQ" 
    insert_document(supabase,hf, binary, subject, topic, question_type, session_id)

def test_query_by_topic():
    print(query_by_topic(supabase,"Capitals", "Geography", hf))
    print(query_by_topic(supabase,"Capitals", "Happy", hf))
    print(query_by_topic(supabase,"Capitals", None, hf))

def test_delete_data_by_session(supabase):
    """
    Test the delete_data_by_session function by deleting all data
    associated with a session_id and checking if it is deleted.
    """
    session_id = get_client_session()
    delete_data_by_session(session_id)
    # Check if data is deleted
    results = supabase.table("document").select("*").eq("session_id", session_id).execute()
    results2 = supabase.table("topic").select("*").eq("session_id", session_id).execute()
    assert not results.data, "Data not deleted"
    assert not results2.data, "Topic data not deleted"



if __name__ == "__main__":
    load_dotenv()
    url: str = os.environ.get("SUPABASE_URL")
    key: str = os.environ.get("SUPABASE_KEY")
    supabase: Client = create_client(url, key)

    model_name = "sentence-transformers/all-MiniLM-L6-v2"
    model_kwargs = {'device': 'cpu'}
    encode_kwargs = {'normalize_embeddings': False}
    hf = HuggingFaceEmbeddings(
        model_name=model_name,
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    #test_query_by_topic()

    #test_delete_data_by_session()